<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>BodyPix Black Background</title>
  <style>
    /* make the canvas fill the viewport and show black behind transparent pixels */
    body, html { margin:0; padding:0; overflow:hidden; height:100%; }
    canvas {
      display: block;
      width: 100vw;
      height: 100vh;
      background-color: black;    /* black shows through any transparent pixels */
      object-fit: cover;
    }
    video { display: none; }       /* hide the raw video element */
  </style>
  <!-- Load TF.js and BodyPix -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.2.0/dist/body-pix.min.js"></script>
</head>
<body>
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas"></canvas>

  <script>
    async function setupCamera() {
      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'environment' }, audio: false
      });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => {
          video.play();
          resolve(video);
        };
      });
    }

    async function main() {
      // 1) start camera
      const video = await setupCamera();

      // 2) prepare our visible canvas
      const canvas = document.getElementById('canvas');
      canvas.width  = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx      = canvas.getContext('2d');

      // 3) an offscreen canvas to hold the mask
      const maskCanvas = document.createElement('canvas');
      maskCanvas.width  = canvas.width;
      maskCanvas.height = canvas.height;
      const maskCtx    = maskCanvas.getContext('2d');

      // 4) load the BodyPix model (MobileNetV1 0.75 for mobile) :contentReference[oaicite:0]{index=0}
      const net = await bodyPix.load({
        architecture: 'MobileNetV1',
        outputStride: 16,
        multiplier: 0.75,
      });

      // 5) the render loop
      async function renderFrame() {
        // a) run segmentation
        const segmentation = await net.segmentPerson(video, {
          internalResolution: 'low',
          segmentationThreshold: 0.5    // lower threshold helps mobile detect more reliably
        });

        // b) convert to an RGBA mask where: background → transparent, person → opaque
        //    toMask(segmentation, maskBackgroundValue, maskForegroundValue) :contentReference[oaicite:1]{index=1}
        const mask = bodyPix.toMask(
          segmentation,
          { r: 0, g: 0, b: 0, a: 0 },    // background pixels become fully transparent
          { r: 0, g: 0, b: 0, a: 255 }   // person pixels become fully opaque (color not used)
        );
        maskCtx.putImageData(mask, 0, 0);

        // c) draw the live video frame
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        // d) mask out everything but the person
        ctx.globalCompositeOperation = 'destination-in';
        ctx.drawImage(maskCanvas, 0, 0, canvas.width, canvas.height);

        // e) reset for next frame
        ctx.globalCompositeOperation = 'source-over';

        requestAnimationFrame(renderFrame);
      }

      renderFrame();
    }

    main().catch(console.error);
  </script>
</body>
</html>
